# <img src="https://github.com/kuanmin/projects/blob/master/welcome.png" width="10%">
**Hello, Welcome!** I am Kuan.<br />
====
Here are some **projects and works** and **examples for actuarial data science and machine learning**. Some are interesting, useful, and delicate, some reach positive and fruitful results, and some are rather challenging. I have learned many skills through the processes and I am very proud of having them in my portfolio.

<!--**Examples of Actuarial Data Science by Swiss Association of Actuaries**
-------
- [x] use CNN to detect anomalies in mortality rates
- [x] use XGBoost and AdaBoost to predict the probability of claims occurrence
- [x] use GLM and ML for claim frequency modeling
- [x] use RNN to predict mortality rate
- [x] use GLM's MLE to speed up NN fitting
- [x] use XAI for claim frequency modeling
- [x] NLP application in client data
- [x] apply dimension reduction and clustering on client data
- [link](https://www.actuarialdatascience.org/ADS-Tutorials)
-->

**Machine Learning Demos**
-------
- [x] data cleaning
- [x] feature extraction
- [x] modeling
- [x] validation
- [x] classification
- [x] clustering
- [x] k-nearest neighbor (KNN)
- [x] Bayesian network
- [x] support vector machine (SVM)
- [link](https://drive.google.com/drive/u/1/folders/1CWdUKd6PY8Q9k2DJ6JzWQ95wn08O6DpR)
(Please open the file with "**Google Caloboratory**" and run on **Jupyter Notebook**.)

**Projects & Works**
-------

(Click on the project name for reports, presentations, and analyses.)


 - [Discount Rate Generator for IFRS 17 and Insurance Capital Standards (ICS 2.0)](https://drive.google.com/drive/folders/1olzbokf25WdQlyB_endGsXgWyBAeL1JF?usp=sharing) (`Python`) (Industrial Implementation)<br />
    - **IFRS 17** is a principles-based standard that requires significant interpretation before it can be implemented in practice. A key consideration is **the discount rate** to be used in measuring liabilities, among other related financial assumptions. As for **Insurance Capital Standards** (**ICS 2.0**), liability portfolios are separated into three “buckets” of decreasing degrees of asset-liability cash-flow matching and consequent recognition of spread.
    - I developed an application to help all insurance companies to address **IFRS** / **ICS** requirements by modeling and implementing **risk-free interest rate** with Python.
    - How to use it?
      1. Download the files “sw_col.xls” and “generator.exe” using the [LINK](https://drive.google.com/drive/folders/1olzbokf25WdQlyB_endGsXgWyBAeL1JF?usp=sharing), and have them in the same folder.
      2. Execute file “generator.exe”, and follow the instruction in each step.
      3. While being asked "Please enter the file name for Smith-Wilson risk free rate and then press ENTER:", enter “sw_col.xls” and then press ENTER.
    - **IFRS 17**, **Insurance Capital Standards** (**ICS 2.0**), **Model Building**, **Discount Rate**, **GUI**
<p align="center">
  <img width="80%" height="80%" src="https://github.com/kuanmin/tii_data/blob/main/present.gif">
</p>


 - [Asset and Liability Modeling for Solvency Test](https://github.com/kuanmin/projects/blob/master/Modeling/Matlab_01.png) (`MATLAB` / *MATLAB*) (Industrial Implementation)<br />
   - Build models such as interest rate, exchange rate, stock price, and rent for asset and liability performance, and forecast and analyze the future performance.
   - Follow with analyses on product providing for different types of guaranteed benefits, and various items of solvency tests.
   - **Solvency Test**, **Model Building**, **Forecast**, **GUI**
   
<p align="center">
  <img width="80%" height="80%" src="https://github.com/kuanmin/projects/blob/master/Modeling/Matlab_01.png">
</p>





 - [SEM-based Customer Contentment Analysis on Mass Merchandiser](https://github.com/kuanmin/projects/tree/master/SEM) (`SAS` / *SAS*) <br />
   - Mass merchandisers need to provide the correct information about their products and services to customers. Thus, it is necessary to have the information to enable customers to meet their real needs and to discover the best way to satisfy and retain customers, as well as to follow consumer sentiment, which can provide early warnings of market conduct and performance. This study seeks to understand ways to retain customers and to identify their levels of contentment with mass merchandisers. The research also focused on helping managers assess and identify the major strengths of the critical success factors of merchandisers, so that the company can sustain and maintain the success it has achieved in the market.
   - In the present work, we study the causal effects from buildings and service to satisfaction, contentment and impression for Costco, Carrefour, and a.mart, using **reliability analysis**, **confirmatory factory analysis** (**CFA**), and **structural equation modeling** (**SEM**) analysis.
   
<p align="center">
  <img width="90%" height="90%" src="https://github.com/kuanmin/projects/blob/master/SEM/SEM.png">
</p>
<p align="center">
  <img width="60%" height="60%" src="https://github.com/kuanmin/projects/blob/master/SEM/model.png">
</p>
 



 - [Forecasting of Time Series based on VAR and Maximum Cross-correlation](https://github.com/kuanmin/projects/tree/master/Thesis%20MSc%20Statistics) (`R`) (Master Thesis - Statistics)<br />
    - Master Thesis - **Forecasting of Time Series based on Vector Autoregression Model and Maximum Cross-correlation**
    - The selection of methods plays an important role in the prediction based on time-series data. In most literature reviews, the vector autoregression model (VAR) has been a popular choice for prediction for many years. There are some disadvantages of this method: (**i**) **the model selection procedure can be really complex**; (**ii**) **the model assumptions are difficult to validate**; (**iii**) **it requires a large amount of data for model building**. The objective of this thesis is to provide an new multivariate-time series prediction method based on the concept of maximum cross-correlation. It requires merely the assumption of “fair linearity” between two time series under investigation. This thesis also compares the proposed method to the vector autoregressive (VAR) model which is widely used in time series analysis with the expectation to provide a new prediction method in practical data analysis. We use data from the Taiwan equity funds and the portfolio of those funds to compare the prediction performances of these two methods. Using the **mean prediction squared errors** (**MPSE**) as assessment criterion, the prediction method based on the maximum cross-correlation best performs under all prediction periods.
    - **Granger causality**, **Vector Autoregression model** (**VAR model**), **Cross-correlation**, **Wald test**, **mean prediction squared error** (**MPSE**)

<p align="center">
  <img width="80%" height="80%" src="https://github.com/kuanmin/projects/blob/master/Thesis%20MSc%20Statistics/formula.png">
</p>


 - [Enhancement of LibKGE](https://github.com/kuanmin/projects/tree/master/Thesis%20MSc%20Data%20Science) (`Python`) (Master Thesis - Data Science)<br />
    - Master Thesis - **Revisiting Ensembles for Knowledge Graph Embeddings**
    - We study the ensembles in KGEs with better trained baselines. Additionally, fine-tune and joint learning were further experimented on ensembles. The study shows that, although ensembles generally outperformed single KGE models with better trained baselines, fine-tuning has shown minor progress comparing to established ensembles. Also, the approaches by joint learning lead to outputs inferior to established ensembles with the same training specification.
    - Open source LibKGE framework is a PyTorch-based library for efficient training, evaluation, and hyperparameter optimization of [knowledge graph embeddings](https://ieeexplore.ieee.org/document/8047276) (KGE). The key goal is to foster reproducible research into (as well as meaningful comparisons between) KGE models and training methods. As the authors argue in [ICLR 2020 paper](https://openreview.net/forum?id=BkxSmlBFvr), the choice of training strategy and hyperparameters are very influential on model performance, often more so than the model class itself.
    - I implement new functions to LibKGE, which will allow the New package to run training and validation, not just on single KGE model, but *multiple models* in one process, so to be able to performe **joint training** and **alternative training models**.
    
 - [Profitability - Statistical Analysis with Excel VBA](https://github.com/kuanmin/projects/tree/master/Profitability%20Analysis%20with%20Excel%20VBA) (`VBA`) <br />
   - Our team of actuaries play a vital role in our organization. Our daily works relate to actuarial analysis, including **Modeling**, **Predicting**, **Profit and Surplus Analysis**, **Risk and Uncertainty**, and **Validation**. And Excel has been applied to parts of these tasks along with the use of VBA. Here are some examples from my work.
   - Click on the images to see the originals. From left to right, up to down: *asset sheet* - *assumption* - *profit analysis* - *sensitivity test*
<p align="center">
  <img width="42%" height="42%" src="https://github.com/kuanmin/projects/blob/master/Profitability%20Analysis%20with%20Excel%20VBA/asset_sheet.png">
  <img width="42%" height="42%" src="https://github.com/kuanmin/projects/blob/master/Profitability%20Analysis%20with%20Excel%20VBA/assumption.png">
</p>

<p align="center">
  <img width="42%" height="42%" src="https://github.com/kuanmin/projects/blob/master/Profitability%20Analysis%20with%20Excel%20VBA/profit_analysis.png">
  <img width="42%" height="42%" src="https://github.com/kuanmin/projects/blob/master/Profitability%20Analysis%20with%20Excel%20VBA/sensitivity_test.png">
</p>
<p align="center">
  <img width="80%" height="80%" src="https://github.com/kuanmin/projects/blob/master/Profitability%20Analysis%20with%20Excel%20VBA/VBA.png">
</p>




 - [Integrating Web Data on Video Games and Companies](https://github.com/kuanmin/projects/tree/master/Integrating%20Web%20Data) (`Python`, `Java`, `XML` / *MapForce*) <br />
   - In this project, we worked with Python and Java libraries BeautifulSoup, Selenium WebDriver, and Jsoup3. And data analytical skills related to **Data Translation**, **Identity Resolution**, and **Data Fusion** are applied.
   - We focus on building an integrated database of video games and video game developers which will be informative to video game players and professionals working in the industry alike. Our combined data can offer interesting new visions that can assist on business decision making and drive video game businesses to a success. Simply by exploring and mining this data, one will be able to gain a better understanding of current video game trends. 
   - For example, our integrated data can provide answers on manifold questions such as ‘Which game platform currently generates the most revenue?’, ‘Which genre types are most popular among the users?’, ’How does game experts’ judgment affect the market sales‘, or ‘How does the revenue of games differ from the worldwide regions?’.
<p align="center">
  <img width="50%" height="50%" src="https://github.com/kuanmin/projects/blob/master/Integrating%20Web%20Data/VideoGames_Shema.png"/>
  <img width="31%" height="31%" src="https://github.com/kuanmin/projects/blob/master/Integrating%20Web%20Data/VideoGames_Shema_2.png"/>
</p>


 - [AI-Based Insurance Broker](https://github.com/kuanmin/projects/tree/master/AI-Based%20Insurance%20Broker) (`Java`, `JSON`, `JavaScript`, *Java AWT*, *NetBeans*, *MongoDB*, *React*, *Flask*) <br />
   - In this project, data knowledge related to **Ontology engineering** and **Multiple-criteria decision analysis** are applied. 
   - We want to shed light onto the current technological state of the insurance broker industry and how AI may transform it. Furthermore, we provide an Recommender System for dental insurances using the **Technique for Order of Preference by Similarity to Ideal Solution** (TOPSIS), a popular **Multiple Criteria Decision Making Method** (MCDM). In addition we design an architectural model which may serve as an example of how to implement an insurance recommender system as a web application with state of the art technology.

<p align="center">
  <img width="80%" height="80%" src="https://github.com/kuanmin/projects/blob/master/AI-Based%20Insurance%20Broker/chain.png"/>
</p>
<p align="center">
  <img width="38%" height="38%" src="https://github.com/kuanmin/projects/blob/master/AI-Based%20Insurance%20Broker/React_01.png"/>
  <img width="38%" height="38%" src="https://github.com/kuanmin/projects/blob/master/AI-Based%20Insurance%20Broker/React_02.png"/>
</p>

 - [Data Infrastructure](https://github.com/kuanmin/projects/tree/master/Data%20Infrastructure) (`SQL` / *PostgreSQL*) <br />
   - While working in the data team of Risk Management department, we deal with financial products and the transactions. Our work is to maintain and improve the data infrastructure of the settlement process. This involves **data pipeline**, **data checks**, **data modeling**, and **data warehouse**. Here are the examples for *data pipeline* and *data model* from our work.
<p align="center">
  <img width="70%" height="80%" src="https://github.com/kuanmin/projects/blob/master/Data%20Infrastructure/data_pipeline.png">
</p>
<p align="center">
  <img width="70%" height="80%" src="https://github.com/kuanmin/projects/blob/master/Data%20Infrastructure/Data_Model.png">
</p>

 - [Recipe Finder](https://github.com/kuanmin/projects/tree/master/Recipe%20Finder) (`Java`, `RDFS`, `SPARQL`, `SQL`, `JavaScript` / *Apache Jena*) <br />
   - "Throw-away society" is a term often used to describe today's society. The German center for nutrition estimates that every year eleven million tonnes of food is thrown away in Germany alone. For the U.S., the figures are even worse: 150,000 tonnes of food is thrown away every day.
   - The biggest reason why ingredients are thrown away is that the quantity initially bought exceeded the actually required quantity for a given period of time. This might be either because of a special promotion, the product not being available in smaller units or just due to a lack of planning for the purchase. One way to reduce some of that waste would be to provide a way to use leftover food.
   - In our project, we achieved just that! We build an API using Apache Jena providing households with an easy way to find recipes incorporating food they need to either consume today or throw away tomorrow. We tapped into the power of the Semantic Web and developed an application which allows its users to browse recipes based on leftovers they might have in their kitchen, ultimately reducing food waste.
   - Apache Jena, a Java framework, is a well-known Semantic Web programming framework. In this project, data knowledge related to **Linked Open Data** and **Ontology engineering** are applied. 
<p align="center">
  <img width="80%" height="80%" src="https://github.com/kuanmin/projects/blob/master/Recipe%20Finder/pics/combined.PNG">
</p>

 - [Information Retrieval on NLTK corpora](https://github.com/kuanmin/projects/tree/master/Information%20Retrieval) (`Python`) <br />
   - In this project, we discuss the implementation of a **Latent Semantic Indexing**-based information retrieval model, and evaluates its performance against the **Vector Space Model** on a collection with 18,828 documents.
   - We implement a LSI-based retrieval system. We expected it to outperform a traditional model like VSM due to its ability to uncover latent structures in the collection of documents. Unfortunately, our results largely did not agree with our expectations, which prompted a deeper analysis of the collection and the evaluation methodology. We found out that LSI heavily depends on the structure of the collection that it is applied on and it would be interesting to repeat the experiment with a different collection with exisiting relevance judgments.




 - [Automated ICD Coding](https://github.com/kuanmin/projects/tree/master/ICD%20matching) (`Python`) <br />
   - To reduce coding errors and cost, this is my attempt applying **Latent Semantic Indexing** to build an ICD coding machine which automatically and accurately translates
the free-text diagnosis descriptions into ICD codes.
<p align="center">
  <img width="80%" height="80%" src="https://github.com/kuanmin/projects/blob/master/ICD%20matching/output_example.png">
</p>

 - [Company Name Matching](https://github.com/kuanmin/projects/tree/master/Company%20Name%20Matching) (`Python`) <br />
   - Some input data are built by handwriting and scanning afterwards or by typing, which might cause data inconsistency and wrong inputs. This will lead to tremendous problems for end users, such as product managers and analysts. We want to avoid any operational inefficiency involving manual correction or misleading statistics or analysis at the further end of the reporting process. Regarding company names, besides typo, different name suffix, such GmbH or Ltd, may cause issue of distinguishability. 
   - This is my attempt applying different type of **Identity Resolution** approaches, such as **jaccard**, **jaro winkler**, **hamming**, **levenshtein**, and **ratcliff obershelp**, to analyse the company name dataset. we also apply **Block methods** with criteria *country* to avoid unnecessary comparisons and reduce quadratic runtime complexity. The goal is to choose an approach to 1.)have the most similar with the similarity up to a threshold, and 2.)make sure the most similar has a certain level of difference between itself and the second most similar, so to perform precise classification.
<p align="center">
  <img width="80%" height="90%" src="https://github.com/kuanmin/projects/blob/master/Company%20Name%20Matching/output_example.png">
</p>
<p align="center">
  <img width="65%" height="80%" src="https://github.com/kuanmin/projects/blob/master/Company%20Name%20Matching/analysis.png">
</p>



 - [Data & Matrix](https://github.com/kuanmin/projects/tree/master/Data%20and%20Matrix) (`R`) <br />
   - In these tasks, data analytical skills related to **Matrix Completion**, **Non-Negative Matrix Factorization**, and **Singular Value Decomposition** are applied.
<p align="center">
  <img width="50%" height="50%" src="https://github.com/kuanmin/projects/blob/master/Data%20and%20Matrix/pics/recovered.png">
</p>


   



   
